
# TÃ¼rkÃ§e Metin Ã–zetleme Modeli (T5 Fine-Tuning)

Bu proje, TÃ¼rkÃ§e metin Ã¶zetleme gÃ¶revi iÃ§in T5 (Text-to-Text Transfer Transformer) modelinin **fine-tuning** iÅŸlemine odaklanmaktadÄ±r. EÄŸitim sÃ¼recinde, [musabg/wikipedia-tr-summarization](https://huggingface.co/datasets/musabg/wikipedia-tr-summarization) veri seti kullanÄ±lmÄ±ÅŸtÄ±r.

## ğŸš€ Ã–zellikler
- **Dil:** TÃ¼rkÃ§e
- **Model:** T5-small
- **Veri Seti:** TÃ¼rkÃ§e metinler ve Ã¶zetlerden oluÅŸan bir veri seti
- **EÄŸitim SÃ¼reci:**
  - BÃ¼yÃ¼k veri setini bellek sÄ±nÄ±rlarÄ±nÄ± aÅŸmamak iÃ§in parÃ§alara bÃ¶ler.
  - KÃ¼Ã§Ã¼k batch boyutlarÄ± ve gradient accumulation kullanÄ±larak bellek verimliliÄŸi saÄŸlanÄ±r.
  - Bellek temizleme iÅŸlemleriyle GPU kullanÄ±m optimizasyonu yapÄ±lÄ±r.
- **DeÄŸerlendirme MetriÄŸi:** ROUGE

## ğŸ”§ Gereksinimler
Bu proje iÃ§in gerekli Python kÃ¼tÃ¼phaneleri:
- `transformers`
- `datasets`
- `accelerate`
- `torch`
- `rouge_score`

Kurulum:
```bash
pip install transformers datasets accelerate rouge_score
```

## ğŸ“– KullanÄ±m
Model, metinleri Ã¶zetlemek iÃ§in aÅŸaÄŸÄ±daki adÄ±mlarÄ± takip eder:

1. **Model ve Tokenizer'Ä± YÃ¼kleyin:**
   ```python
   from transformers import T5Tokenizer, T5ForConditionalGeneration

   tokenizer = T5Tokenizer.from_pretrained("path/to/fine_tuned_model")
   model = T5ForConditionalGeneration.from_pretrained("path/to/fine_tuned_model")
   ```

2. **Bir Metni Ã–zetleyin:**
   ```python
   input_text = "TÃ¼rkÃ§e Ã¶zetleme yapmak iÃ§in bu metni kullanabilirsiniz."
   inputs = tokenizer(input_text, return_tensors="pt", max_length=256, truncation=True, padding="max_length")
   summary_ids = model.generate(inputs["input_ids"], max_length=64, num_beams=4, early_stopping=True)
   predicted_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
   print("Ã–zetlenmiÅŸ Metin:", predicted_summary)
   ```

## ğŸ‹ï¸â€â™‚ï¸ EÄŸitim SÃ¼reci
- Model, eÄŸitim iÃ§in parÃ§alanmÄ±ÅŸ veri seti Ã¼zerinde her parÃ§a ayrÄ± ayrÄ± iÅŸlenerek eÄŸitilmiÅŸtir.
- Her parÃ§a eÄŸitildikten sonra model kaydedilmiÅŸ ve bellek temizlenmiÅŸtir.

**AdÄ±mlar:**
1. Veri seti parÃ§alanÄ±r (Ã¶r. 10.000 satÄ±rlÄ±k parÃ§alara).
2. Her parÃ§a ayrÄ± ayrÄ± tokenleÅŸtirilir.
3. T5-small modeli her parÃ§a Ã¼zerinde eÄŸitilir.
4. Bellek optimizasyonu iÃ§in `torch.cuda.empty_cache()` kullanÄ±lÄ±r.

## ğŸ“‚ Proje YapÄ±sÄ±
- `fine_tuning_dataset_split.py`: EÄŸitim iÅŸlemini parÃ§alara bÃ¶len ve GPU belleÄŸi optimizasyonuyla eÄŸitim yapan Python kodu.
- `results/`: EÄŸitimden elde edilen model sonuÃ§larÄ±.
- `logs/`: EÄŸitim sÄ±rasÄ±nda oluÅŸan log dosyalarÄ±.

## ğŸ“Š DeÄŸerlendirme
- EÄŸitimden sonra model, ROUGE metrikleri ile deÄŸerlendirilmiÅŸtir.

## ğŸ“œ Lisans
Bu proje **MIT lisansÄ±** ile paylaÅŸÄ±lmaktadÄ±r.
